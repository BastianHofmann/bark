{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Combining Reinforcement Learning with Monte Carlo Tree Search\n",
    "\n",
    "## Motivation\n",
    "Deep Reinforcement Learning is used to learn a mapping between the observed state of the world and the best action to take especially when dealing with constantly changing and eventually high dimensional worlds. This mapping is learned without any prior knowledge of that world. To do so, a Deep Neural Network is trained offline using rewards and penalizations. Once trained, the network can be used to act online and is guaranteed to be sample efficient and ensure real-time capability, since no time-limited optimization is performed during execution. Such a behavior is of course desired when dealing with autonomous cars that have to act and react fast to ensure safety and efficiency.\n",
    "\n",
    "On the other hand, we have decision processes such as Monte Carlo Tree Search which are informed search algorithms that do not have a training phase. Instead, they use a heuristic to find the best action to take. One major challenge with vanilla MCTS is the scalability to larger environments with long episodes, which renders the algorithm uncapable of finding a solution in real time.\n",
    "\n",
    "You might ask, well if DRL is real-time capable on its own like we want it to be, why are we even talking about MCTS. The problem with DRL is that it can be unrobust when a completely new scenario is encountered, and such an unexpected behavior can be dangerous. A new AlphaGO implementation that combines DRL with Monte Carlo Tree Search has shown  great improvements in the performance. For this reason, we want to implement that idea in our project and see if we can achieve better results when applying it in the case of autonomous driving. That is why we will use the Q-values from the DRL agent as heuristic to improve the performance of the tree search by better guiding the search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "We first start working on Bark-ML and using Python we train a new DRL agent known as the categorial DQN. Because getting the information about the world in the vicinity of our ego car is a speed-critical part of the project, we use C++ instead of Python to get the observation of the world, since C++ has a much faster performance.  \n",
    "Afterwards, we save the trained Q-Network in Python and load it using C++ for later evaluations that are time-constrained.\n",
    "\n",
    "In parallel, we work in Bark-planner-mcts and create in C++ three different heuristic functions that influence the choice of moves in the tree search.\n",
    "\n",
    "Finally, we use Bark to evaluate our results in Python but by calling the C++ observer and the Q-network evaluator from bark-ML and access the different heuristics from planner-mcts.\n",
    "So now letâ€™s discuss each step into more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Model Saving\n",
    "The first step is to train the CDQN Agent. This is achieved by doing the following steps:\n",
    "\n",
    "1. Clone bark-ml from https://github.com/SebastianGra/bark-ml_MCTS_RL and follow the steps to create a virtual environment like stated in the link and then activate it.\n",
    "\n",
    "2. Change the path where the checkpoints, summaries and model are saved to your local path in the lines 43, 44 and 45 in the examples/tfa.py respectivly examples/tfa_discrete.py file.\n",
    "\n",
    "3. In bark_ml/behaviors/discrete_behavior.py, change the number of possible discrete actions. This will later influence how long the tree search takes to go through the possible actions. For a faster results, reduce the number of actions from 10 and 5 in lines 25 and 29 respectively to 4 and 2. This results in 8 possible combinations of actions that the ego vehicle can perform.\n",
    "\n",
    "4. In bark_ml/library_wrappers/lib_tf_agents/tfa_wrapper.py, go to line 22 and change the dtype from float32 to `int32`.\n",
    "\n",
    "5. And finally, train the agent using the command line `bazel run //examples:tfa -- --mode=train`. This will also save the Q-network, which we later need for the tree search to evaluate the Q-values given the state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "An important part of our work is the testing. For our project you can test both the model loader and the observer we have implemented.\n",
    "\n",
    "To test the **model loader** do the following:\n",
    "\n",
    "1. In the file bark_ml/tests/model_loader_test.cc file, change the path in line 15 to the same local path used before to save the model.\n",
    "\n",
    "2. Git clone the Tensorflow library from this link https://github.com/steven-guo94/libtensorflow_so\n",
    "\n",
    "3. Run this command in the terminal `export LD_LIBRARY_PATH=/.../libtensorflow_so/libtensorflow/lib` by adding the local path to where the Tensorflow library is.\n",
    "\n",
    "4. Run this command in the terminal to test the saved model `bazel run //bark_ml/tests:model_loader_test`.\n",
    "\n",
    "For testing the **observer** an additional more comprehensive observer test in c++ has been implemented. It verifies several properties of the observer:\n",
    "\n",
    "- The correct length of the concatenated state-array according a given number of observed agents.\n",
    "- Agents too far away from the ego position are filtered out.\n",
    "- That the observed agent states are stacked in the correct order into the concatenates state-array (closest (ego) first, most far one last).\n",
    "- The normalization of the state-values works correctly.\n",
    "\n",
    "To exectute the test together with the new obsever do following:\n",
    "\n",
    "1. Ensure that line 11 in observer_test.cc says `#include \"bark_ml/observers/nearest_observer.hpp\"` so it uses the correct observer.\n",
    "\n",
    "2. Execute the test via command `bazel run //bark_ml/tests:observer_test`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search with Heuristics\n",
    "\n",
    "As mentioned in Motivation, MCTS uses different heuristics to find the best action to take. You can access the different heuristics from planner-mcts.\n",
    "\n",
    "To play with **planner-mcts** do the following:\n",
    "\n",
    "1. Get the code locally: `git clone https://github.com/bark-simulator/planner-mcts.git`. \n",
    "\n",
    "2. Switch to the branch which we are working on: `git checkout migrate_to_new_bark`.\n",
    "\n",
    "3. Set up the vitual environment: `bash util/setup_test_venv.sh`.\n",
    "\n",
    "4. Get into the venv: `source util/into_test_venv.sh`.\n",
    "\n",
    "In our project, three heuristics are used:\n",
    "\n",
    "1. **Random heuristic**: which is defined here https://github.com/juloberno/mamcts/blob/master/mcts/heuristics/random_heuristic.h .\n",
    "\n",
    "2. **Domain heuristic**: which is defined here https://github.com/bark-simulator/planner-mcts/blob/migrate_to_new_bark/bark_mcts/models/behavior/heuristics/domain_heuristic.hpp . To test the domain heuristic run this command in terminal: `bazel test //bark_mcts/models/behavior/tests:single_agent_domain_heuristic_test`.\n",
    "    \n",
    "3. **NN heuristic**: which is defined here https://github.com/bark-simulator/planner-mcts/blob/migrate_to_new_bark/bark_mcts/models/behavior/heuristics/nn_heuristic.hpp . To test the NN heuristic run this command in terminal: `bazel test //bark_mcts/models/behavior/tests:single_agent_nn_heuristic_test`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Systematically benchmarking behavior consists of\n",
    "1. A reproducable set of scenarios (we call it **BenchmarkDatabase**)\n",
    "2. Metrics, which you use to study the performance (we call it **Evaluators**)\n",
    "3. The behavior model(s) under test\n",
    "\n",
    "To run this benchmarking notebook, you should run the following commands in terminal:\n",
    "1. Download the bark repository: `git clone https://github.com/Lizhu-Chen/bark.git`.\n",
    "    \n",
    "2. Download the libtensorflow: `git clone https://github.com/steven-guo94/libtensorflow_so.git`.\n",
    "    \n",
    "3. Export the local path of libtensorflow: `export LD_LIBRARY_PATH=your local path of libtensorflow`.\n",
    "\n",
    "4. Open bark in the terminal, and switch to the branch: `git branch practical_course_mcts_rl`.\n",
    "    \n",
    "5. Get into vitual environment: `bash install.sh` and then `source dev_into.sh`.\n",
    "    \n",
    "6. Run the notebook: `bazel run //docs/tutorials:run --define planner_uct=true`.\n",
    "    \n",
    "\n",
    "Our **BenchmarkRunner** can then run the benchmark and produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BehaviorUCTSingleAgentMacroActions' from 'bark.core.models.behavior' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-664c4ef8d456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBehaviorUCTSingleAgentMacroActions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BehaviorUCTSingleAgentMacroActions' from 'bark.core.models.behavior' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unittest\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "\n",
    "from bark.runtime.scenario.scenario import *\n",
    "from benchmark_database.load.benchmark_database import BenchmarkDatabase\n",
    "from benchmark_database.serialization.database_serializer import DatabaseSerializer\n",
    "from bark.benchmark.benchmark_runner import BenchmarkRunner, BenchmarkConfig, BenchmarkResult\n",
    "from bark.benchmark.benchmark_runner_mp import BenchmarkRunnerMP\n",
    "from bark.benchmark.benchmark_analyzer import BenchmarkAnalyzer\n",
    "\n",
    "from bark.core.world.evaluation import *\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "\n",
    "from bark.runtime.viewer.matplotlib_viewer import MPViewer\n",
    "from bark.runtime.viewer.video_renderer import VideoRenderer\n",
    "\n",
    "\n",
    "from bark.core.models.behavior import BehaviorUCTSingleAgentMacroActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "The benchmark database provides a reproducable set of scenarios.\n",
    "A scenario get's created by a ScenarioGenerator (we have a couple of them). The scenarios are serialized into binary files (ending `.bark_scenarios`) and packed together with the map file and the parameter files into a `.zip`-archive. We call this zipped archive a relase, which can be published at Github, or processed locally, which named `benchmark_database_test.zip`. \n",
    "It's usually saved in `bark/bazel-bin/examples/benchmark_database.runfiles/benchmark_database_release/database` folder, you can find the local path of the `.zip` and change `dbs.process(\"your local path of the .zip file\")` \n",
    "\n",
    "\n",
    "\n",
    "## We will first start with the DatabaseSerializer\n",
    "\n",
    "The **DatabaseSerializer** recursively serializes all scenario param files sets\n",
    " within a folder.\n",
    " \n",
    "We will process the database directory from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = DatabaseSerializer(test_scenarios=2, test_world_steps=2, num_serialize_scenarios=10)\n",
    "dbs.process(\"/home/vivienne/Praktikum/fork/bark/bazel-bin/modules/benchmark/tests/py_benchmark_process_tests.runfiles/benchmark_database\")\n",
    "local_release_filename = dbs.release(version=\"test\")\n",
    "\n",
    "print('Filename:', local_release_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reload to test correct parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = BenchmarkDatabase(database_root=local_release_filename)\n",
    "scenario_generation, _ = db.get_scenario_generator(scenario_set_id=0)\n",
    "\n",
    "for scenario_generation, _ in db:\n",
    "  print('Scenario: ', scenario_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators\n",
    "\n",
    "Evaluators allow to calculate a boolean, integer or real-valued metric based on the current simulation world state.\n",
    "\n",
    "The current evaluators available in BARK are:\n",
    "- StepCount: returns the step count the scenario is at.\n",
    "- GoalReached: checks if a controlled agentâ€™s Goal Definitionis satisfied.\n",
    "- DrivableArea: checks whether the agent is inside its RoadCorridor.\n",
    "- Collision(ControlledAgent): checks whether any agent or only the currently controlled agent collided\n",
    "\n",
    "Let's now map those evaluators to some symbols, that are easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {\"success\" : \"EvaluatorGoalReached\", \\\n",
    "              \"collision\" : \"EvaluatorCollisionEgoAgent\", \\\n",
    "              \"max_steps\": \"EvaluatorStepCount\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the terminal conditions of our benchmark. We state that a scenario ends, if\n",
    "- a collision occured\n",
    "- the number of time steps exceeds the limit\n",
    "- the definition of success becomes true (which we defined to reaching the goal, using EvaluatorGoalReached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_when = {\"collision\" :lambda x: x, \\\n",
    "                 \"max_steps\": lambda x : x>40, \\\n",
    "                 \"success\" : lambda x: x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviors Under Test\n",
    "Let's now define the parameters for different heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_param_file =\"macro_action_params.json\" # must be within examples params folder\n",
    "params1 = ParameterServer(filename= os.path.join(\"examples/mcts_rl/params/\",scenario_param_file))\n",
    "params2 = ParameterServer(filename= os.path.join(\"examples/mcts_rl/params/\",scenario_param_file))\n",
    "params3 = ParameterServer(filename= os.path.join(\"examples/mcts_rl/params/\",scenario_param_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of **Random Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1[\"BehaviorUctSingleAgent\"][\"UseRandomHeuristic\"]=True\n",
    "params1[\"BehaviorUctSingleAgent\"][\"UseNNHeuristic\"]=False\n",
    "params1[\"BehaviorUctSingleAgent\"][\"Mcts\"][\"UctStatistic\"][\"ReturnLowerBound\"] = -10000.0\n",
    "params1[\"BehaviorUctSingleAgent\"][\"Mcts\"][\"UctStatistic\"][\"ReturnUpperBound\"] = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of **Domain Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2[\"BehaviorUctSingleAgent\"][\"UseRandomHeuristic\"]=False\n",
    "params2[\"BehaviorUctSingleAgent\"][\"UseNNHeuristic\"]=False\n",
    "params2[\"BehaviorUctSingleAgent\"][\"Mcts\"][\"UctStatistic\"][\"ReturnLowerBound\"] = -10000.0\n",
    "params2[\"BehaviorUctSingleAgent\"][\"Mcts\"][\"UctStatistic\"][\"ReturnUpperBound\"] = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of **NN Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2[\"BehaviorUctSingleAgent\"][\"UseRandomHeuristic\"]=False\n",
    "params2[\"BehaviorUctSingleAgent\"][\"UseNNHeuristic\"]=True\n",
    "params2[\"BehaviorUctSingleAgent\"][\"Mcts\"][\"UctStatistic\"][\"ReturnLowerBound\"] = -10000.0\n",
    "params2[\"BehaviorUctSingleAgent\"][\"Mcts\"][\"UctStatistic\"][\"ReturnUpperBound\"] = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same behavior for the three heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_tested = {\"RandomHeuristic\": BehaviorUCTSingleAgentMacroActions(params1),\"DomainHeuristic\": BehaviorUCTSingleAgentMacroActions(params2),\"NNHeuristic\": BehaviorUCTSingleAgentMacroActions(params3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Runner\n",
    "\n",
    "The BenchmarkRunner allows to evaluate behavior models with different parameter configurations over the entire benchmarking database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_runner = BenchmarkRunner(benchmark_database=db,\\\n",
    "                                   evaluators=evaluators,\\\n",
    "                                   terminal_when=terminal_when,\\\n",
    "                                   behaviors=behaviors_tested,\\\n",
    "                                   log_eval_avg_every=10)\n",
    "\n",
    "result = benchmark_runner.run(maintain_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now dump the files, to allow them to be postprocessed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dump(os.path.join(\"./benchmark_results.pickle\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results\n",
    "\n",
    "Benchmark results contain\n",
    "- the evaluated metrics of each simulation run, as a Panda Dataframe\n",
    "- the world state of every simulation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_loaded = BenchmarkResult.load(os.path.join(\"./benchmark_results.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now first analyze the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_loaded.get_data_frame()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Analyzer\n",
    "\n",
    "The benchmark analyzer allows to filter the results to show visualize what really happened. These filters can be set via a dictionary with lambda functions specifying the evaluation criteria which must be fullfilled.\n",
    "\n",
    "A config is basically a simulation run, where step size, controlled agent, terminal conditions and metrics have been defined.\n",
    "\n",
    "Let us first load the results into the BenchmarkAnalyzer and then filter the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = BenchmarkAnalyzer(benchmark_result=result_loaded)\n",
    "\n",
    "\n",
    "configs_rd = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"RandomHeuristic\", \"success\": lambda x : not x})\n",
    "configs_dm = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"DomainHeuristic\", \"success\": lambda x : not x})\n",
    "configs_nn = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"NNHeuristic\", \"success\": lambda x : not x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a video from them. We will use Matplotlib Viewer and render everything to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15, 15])\n",
    "viewer = MPViewer(x_range=[-75, 75],\n",
    "                  y_range=[-75, 75],\n",
    "                  follow_agent_id=True)\n",
    "video_exporter = VideoRenderer(renderer=viewer, world_step_time=0.2)\n",
    "\n",
    "analyzer.visualize(viewer = video_exporter, real_time_factor = 1, configs_idx_list=configs_dm[1:3], fontsize=6)\n",
    "                   \n",
    "video_exporter.export_video(filename=\"./heuristic_test_video.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}