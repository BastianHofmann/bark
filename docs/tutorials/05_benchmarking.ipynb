{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Behavior Planners in BARK\n",
    "\n",
    "This notebook the benchmarking workflow of BARK.\n",
    "\n",
    "Systematically benchmarking behavior consists of\n",
    "1. A reproducable set of scenarios (we call it **BenchmarkDatabase**)\n",
    "2. Metrics, which you use to study the performance (we call it **Evaluators**)\n",
    "3. The behavior model(s) under test\n",
    "\n",
    "Our **BenchmarkRunner** can then run the benchmark and produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'benchmark_database'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ceac042ac17e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbenchmark_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_database\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBenchmarkDatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbenchmark_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase_serializer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatabaseSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_runner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBenchmarkRunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBenchmarkConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBenchmarkResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'benchmark_database'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "\n",
    "from load.benchmark_database import BenchmarkDatabase\n",
    "from serialization.database_serializer import DatabaseSerializer\n",
    "from bark.benchmark.benchmark_runner import BenchmarkRunner, BenchmarkConfig, BenchmarkResult\n",
    "from bark.benchmark.benchmark_analyzer import BenchmarkAnalyzer\n",
    "\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "\n",
    "from bark.runtime.viewer.matplotlib_viewer import MPViewer\n",
    "from bark.runtime.viewer.video_renderer import VideoRenderer\n",
    "\n",
    "\n",
    "from bark.models.behavior import BehaviorIDMClassic, BehaviorConstantVelocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "The benchmark database provides a reproducable set of scenarios. A scenario get's created by a ScenarioGenerator (we have a couple of them). The scenarios are serialized into binary files (ending .bark_scenarios) and packed together with the map file and the parameter files into a .zip-archive. We call this zipped archive a relase, which can be published at Github, or processed locally. \n",
    "\n",
    "\n",
    "\n",
    "## We will first start with the DatabaseSerializer\n",
    "\n",
    "The **DatabaseSerializer** recursively serializes all scenario param files sets\n",
    " within a folder.\n",
    " \n",
    "We will process the database directory from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbs = DatabaseSerializer(test_scenarios=4, test_world_steps=5, num_serialize_scenarios=10)\n",
    "dbs = DatabaseSerializer(test_scenarios=1, test_world_steps=10, num_serialize_scenarios=1)\n",
    "dbs.process(\"../../../benchmark_database/data/database1\")\n",
    "local_release_filename = dbs.release(version=\"tutorial\")\n",
    "\n",
    "print('Filename:', local_release_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reload to test correct parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = BenchmarkDatabase(database_root=local_release_filename)\n",
    "scenario_generation, _ = db.get_scenario_generator(scenario_set_id=0)\n",
    "\n",
    "for scenario_generation, _ in db:\n",
    "  print('Scenario: ', scenario_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators\n",
    "\n",
    "Evaluators allow to calculate a boolean, integer or real-valued metric based on the current simulation world state.\n",
    "\n",
    "The current evaluators available in BARK are:\n",
    "- StepCount: returns the step count the scenario is at.\n",
    "- GoalReached: checks if a controlled agentâ€™s Goal Definitionis satisfied.\n",
    "- DrivableArea: checks whether the agent is inside its RoadCorridor.\n",
    "- Collision(ControlledAgent): checks whether any agent or only the currently controlled agent collided\n",
    "\n",
    "Let's now map those evaluators to some symbols, that are easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {\"success\" : \"EvaluatorGoalReached\", \\\n",
    "              \"collision\" : \"EvaluatorCollisionEgoAgent\", \\\n",
    "              \"max_steps\": \"EvaluatorStepCount\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the terminal conditions of our benchmark. We state that a scenario ends, if\n",
    "- a collision occured\n",
    "- the number of time steps exceeds the limit\n",
    "- the definition of success becomes true (which we defined to reaching the goal, using EvaluatorGoalReached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_when = {\"collision\" :lambda x: x, \\\n",
    "                 \"max_steps\": lambda x : x>40, \\\n",
    "                 \"success\" : lambda x: x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviors Under Test\n",
    "Let's now define the parameters for different heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParameterServer() \n",
    "behaviors_tested = {\"IDM\": BehaviorIDMClassic(params), \"Const\" : BehaviorConstantVelocity(params)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Runner\n",
    "\n",
    "The BenchmarkRunner allows to evaluate behavior models with different parameter configurations over the entire benchmarking database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_runner = BenchmarkRunner(benchmark_database=db,\\\n",
    "                                   evaluators=evaluators,\\\n",
    "                                   terminal_when=terminal_when,\\\n",
    "                                   behaviors=behaviors_tested,\\\n",
    "                                   log_eval_avg_every=10)\n",
    "\n",
    "result = benchmark_runner.run(maintain_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now dump the files, to allow them to be postprocessed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.dump(os.path.join(\"./benchmark_results.pickle\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results\n",
    "\n",
    "Benchmark results contain\n",
    "- the evaluated metrics of each simulation run, as a Panda Dataframe\n",
    "- the world state of every simulation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_loaded = BenchmarkResult.load(os.path.join(\"./benchmark_results.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now first analyze the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_loaded.get_data_frame()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Analyzer\n",
    "\n",
    "The benchmark analyzer allows to filter the results to show visualize what really happened. These filters can be set via a dictionary with lambda functions specifying the evaluation criteria which must be fullfilled.\n",
    "\n",
    "A config is basically a simulation run, where step size, controlled agent, terminal conditions and metrics have been defined.\n",
    "\n",
    "Let us first load the results into the BenchmarkAnalyzer and then filter the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = BenchmarkAnalyzer(benchmark_result=result_loaded)\n",
    "\n",
    "\n",
    "configs_idm = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"IDM\", \"success\": lambda x : not x})\n",
    "configs_const = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"Const\", \"success\": lambda x : not x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a video from them. We will use Matplotlib Viewer and render everything to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_step_time=5\n",
    "\n",
    "params2 = ParameterServer()\n",
    "\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "viewer = MPViewer(params=params2, y_length = 80, enforce_y_length=True, enforce_x_length=False,\\\n",
    "                  follow_agent_id=True, axis=fig.gca())\n",
    "video_exporter = VideoRenderer(renderer=viewer, world_step_time=sim_step_time)\n",
    "\n",
    "analyzer.visualize(viewer = video_exporter, configs_idx_list=configs_idm[1:3], \\\n",
    "                  real_time_factor=10, fontsize=6)\n",
    "                   \n",
    "video_exporter.export_video(filename=\"./tutorial_video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"./tutorial_video.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}